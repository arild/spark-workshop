<!--
Google IO 2012/2013 HTML5 Slide Template

Authors: Eric Bidelman <ebidel@gmail.com>
         Luke Mah√© <lukem@google.com>

URL: https://code.google.com/p/io-2012-slides
-->
<!DOCTYPE html>
<html>
<head>
    <title></title>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <!--<meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">-->
    <!--<meta name="viewport" content="width=device-width, initial-scale=1.0">-->
    <!--This one seems to work all the time, but really small on ipad-->
    <!--<meta name="viewport" content="initial-scale=0.4">-->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <link rel="stylesheet" media="all" href="theme/css/default.css">
    <link rel="stylesheet" media="only screen and (max-device-width: 480px)" href="theme/css/phone.css">
    <link rel="stylesheet" media="all" href="custom.css">
    <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
    <script data-main="js/slides" src="js/require-1.0.8.min.js"></script>
</head>
<body style="opacity: 0">

<slides class="layout-widescreen">

    <slide class="title-slide segue nobackground">
        <aside class="gdbar"><img src="images/spark-logo-hd.png"></aside>
	<aside class="gdbar right bottom" style="background: #FFF"><img src="images/visma.png" style="height: 60px"></aside>

        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
        <hgroup class="auto-fadein">
            <h1 data-config-title><!-- populated from slide_config.json --></h1>
            <h2 data-config-subtitle><!-- populated from slide_config.json --></h2>
            <p data-config-presenter><!-- populated from slide_config.json --></p>
        </hgroup>
    </slide>

    <slide class="thank-you-slide">
        <aside class="note">
            <section>
                <ul>
                    <li>Example code presented in the slides will be in scala.</li>
                    <li>All the example code can be found in both the Scala and Java branch.</li>
                </ul>
            </section>
        </aside>
        <article class="flexbox vleft">
            <ul>
                <li>Code: <a href="https://github.com/arild/spark-workshop">https://github.com/arild/spark-workshop</a></li>
                <li>
                    <ul>
                        <li>'master' is starting point for exercises</li>
                        <li>'solution' for solution</li>
                    </ul>
                </li>
            </ul>

            <ul>
                <li>Slides: <a href="http://arild.github.com/spark-workshop">http://arild.github.com/spark-workshop</a></span></li>
                <li>
                    <ul>
                        <li>Or /slides/index.html in 'master' branch</li>
                    </ul>
                </li>

            </ul>
        </article>
    </slide>

    <slide>
        <hgroup>
            <h2>What is Spark?</h2>
        </hgroup>
        <article class="large">
            <img height="500px" src="figures/spark-architecture.png"/>
        </article>
    </slide>

    <slide>
        <hgroup>
            <h2>Outline</h2>
        </hgroup>
        <article class="large">
            <ul>
                <li class="outlineHighlight">Part 1 - Spark Core</li>
                <li>Part 2 - Spark SQL</li>
                <li>Part 3 - Spark Streaming</li>
            </ul>
        </article>
    </slide>

    <slide>
        <hgroup>
            <h2>RDD (Resillient Distributed Dataset)</h2>
        </hgroup>
        <article class="large">
            <ul>
                <li>Core concept in Spark</li>
                <li>Spark automatically distributes data and parallelizes operations on RDDs</li>
                <li>Transformations vs. actions</li>
            </ul>
            <img width="700px" src="figures/transformation-vs-action.png" />
            </article>
    </slide>

    <slide>
        <hgroup>
            <h2>Reading and filtering files</h2>
        </hgroup>
        <article class="large">
            
            <pre class="prettyprint">val sc = new SparkContext("local[4]", "AppName")

val log: RDD[AccessLogRecord] = sc.textFile("access_log-1")
  .map(AccessLogParser.parseRecord) // Transformation (returns new RDD)

val result: Array[AccessLogRecord] = log
  .filter(record => record.bytesSent > 1024) // Transformation (returns new RDD)
  .collect() // Action (compute result)</pre>
        </article>
    </slide>

    <slide>
        <hgroup>
            <h2>Set operations</h2>
        </hgroup>
        <article class="large">
            <pre class="prettyprint">val sc = new SparkContext("local[4]", "AppName")

val log1: RDD[AccessLogRecord] = sc.textFile("access_log-1")
  .map(AccessLogParser.parseRecord)
val log2: RDD[AccessLogRecord] = sc.textFile("access_log-2")
  .map(AccessLogParser.parseRecord)

log1.intersection(log2) // Transformation (returns new RDD)
  .foreach(record => {
    // Write to external storage e.g.
  })</pre>
        </article>
    </slide>

    <slide>
        <hgroup>
            <h2>Word count</h2>
        </hgroup>
        <article class="large">
            <pre class="prettyprint">val sc = new SparkContext("local[4]", "AppName")

val statusCodes: Array[(Int, Int)] = sc.textFile("access_log-1")
  .map(AccessLogParser.parseRecord(_).status)
  .map(status => (status, 1))
  .reduceByKey((a, b) => a + b)
  .take(10)</pre>
        </article>
    </slide>


    <slide>
        <hgroup>
            <h2>Part 1 - Exercises</h2>
        </hgroup>
        <article class="large">
            <ul>
               <li>git clone <a href="https://github.com/arild/spark-workshop.git">https://github.com/arild/spark-workshop.git</a></li>
               <li>Implement LogAnalyzer</li>
               <li>Make tests in LogAnalyzerTest green</li>
            </ul>
            <br>

            <ul>
                <li>Docs:
                    <ul>
                        <li><a style="margin-left: 1em" href="http://spark.apache.org/docs/1.3.1/programming-guide.html">Spark Core programming guide</a></li>
                        <li><a style="margin-left: 1em" href="http://spark.apache.org/docs/1.3.1/programming-guide.html#transformations">Transformations & Actions</a></li>
                    </ul>
                </li>
            </ul>
        </article>
    </slide>

    <slide>
        <hgroup>
            <h2>Running tests in SBT</h2>
        </hgroup>
        <article class="large">
            <ul>
                <li>Run all tests<br /><br />
                <pre class="prettyprint">$ ./sbt
> test</pre>
                </li>
                <li>Autorun (~) part1 tests<br /><br />
                    <pre class="prettyprint">$ ./sbt
> ~test-only workshop.part1.*</pre></li>
            </ul>
        </article>
    </slide>

    <slide>
        <hgroup>
            <h2>Outline</h2>
        </hgroup>
        <article class="large">
            <ul>
                <li>Part 1 - Spark Core</li>
                <li class="outlineHighlight">Part 2 - Spark SQL</li>
                <li>Part 3 - Spark Streaming</li>
            </ul>
        </article>
    </slide>

    <slide>
        <hgroup>
            <h2>Creating DataFrame with schema</h2>
        </hgroup>
        <article class="large">
                        <pre class="prettyprint">val sc = new SparkContext("local[4]", "AppName")
val sqlContext = new SQLContext(sc)

// Used to implicitly convert an RDD to a DataFrame
import sqlContext.implicits._

val df: DataFrame = sc.textFile("access_log-1")
  .map(AccessLogParser.parseRecord)
  .toDF()
df.printSchema()</pre>
        </article>
        <article class="smaller"><pre class="prettyprint">root
 |-- ipAddress: string (nullable = true)
 |-- rfc1413ClientIdentity: string (nullable = true)
 |-- remoteUser: string (nullable = true)
 |-- dateTime: string (nullable = true)
 |-- request: string (nullable = true)
 |-- status: integer (nullable = false)
 |-- bytesSent: integer (nullable = false)
</pre>
        </article>
    </slide>

    <slide>
        <hgroup>
            <h2>Writing SQL</h2>
        </hgroup>
        <article class="large">
                        <pre class="prettyprint">val sc = new SparkContext("local[4]", "AppName")
val sqlContext = new SQLContext(sc)

import sqlContext.implicits._ // implicitly convert an RDD to a DataFrame

val df: DataFrame = sc.textFile("access_log-1")
  .map(AccessLogParser.parseRecord).toDF()

df.registerTempTable("logs")

val res: Array[Row] = df.sqlContext.sql(
  """SELECT ipAddress
     FROM logs
     WHERE bytesSent > 1024
  """).collect()

res.foreach(row => println(row.getString(0)))</pre>
        </article>
    </slide>

    <slide>
        <hgroup>
            <h2>Tables</h2>
        </hgroup>
        <article class="large">
                        <pre class="prettyprint">case class AccessLogRecord (
    ipAddress: String,
    rfc1413ClientIdentity: String,   // typically `-`
    remoteUser: String,              // typically `-`
    dateTime: String,                // [day/month/year:hour:minute:second zone]
    request: String,                 // `GET /foo ...`
    status: Int,                     // 200, 404, etc.
    bytesSent: Int
)
</pre>
        </article>
        <article class="large">
                        <pre class="prettyprint">case class HttpStatusCode(
    status: Int,         // 200, 404, etc.
    description: String, // "OK", "Not Found", etc.
    status_type: String  // "Successful", "Client Error", etc.
)

</pre>
        </article>
    </slide>


    <slide>
        <hgroup>
            <h2>Part 2 - Exercises</h2>
        </hgroup>
        <article class="large">
            <ul>
                <li>Implement LogAnalyzerSql</li>
                <li>Make tests in LogAnalyzerSqlTest green</li>
            </ul>
            <br>

            <ul>
                <li>Docs:
                    <ul>
                        <li><a style="margin-left: 1em" href="https://spark.apache.org/docs/1.3.1/sql-programming-guide.html">Spark SQL and DataFrame Guide</a></li>
                    </ul>
                </li>
            </ul>
        </article>
    </slide>


    <slide>
        <hgroup>
            <h2>Part 3 - Exercises</h2>
        </hgroup>
        <article class="large">
            <ul>
                <li>Implement Xyz</li>
                <li>Make tests in XyzTest green</li>
            </ul>
            <br>

            <ul>
                <li>Docs:
                    <ul>
                        <li>Spark Core<a style="margin-left: 1em" href="http://spark.apache.org/docs/1.2.0/programming-guide.html">http://spark.apache.org/docs/1.2.0/programming-guide.html</a></li>
                    </ul>
                </li>
            </ul>
        </article>
    </slide>

    <slide class="thank-you-slide segue nobackground">
        <article class="flexbox vleft">
            <h2>Thanks!</h2>
            <br><br>
            <h2>Questions?</h2>
        </article>
        <p class="auto-fadein" data-config-contact>
            <!-- populated from slide_config.json -->
        </p>
    </slide>


    <slide class="backdrop"></slide>

    </slides>

<script>
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-XXXXXXXX-1']);
_gaq.push(['_trackPageview']);

(function() {
  var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
  ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
  var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>

<!--[if IE]>
<script src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js"></script>
<script>CFInstall.check({mode: 'overlay'});</script>
<![endif]-->
</body>
</html>
